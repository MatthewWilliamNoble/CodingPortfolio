{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "e81ee64d-e474-4662-9036-ce23df615199",
        "_uuid": "b6269c0e8f417f82daf093dda8fa0da6d2c57d86"
      },
      "cell_type": "markdown",
      "source": "# Introduction\n**This will be your workspace for Kaggle's Machine Learning education track.**\n\nYou will build and continually improve a model to predict housing prices as you work through each tutorial.  Fork this notebook and write your code in it.\n\nThe data from the tutorial, the Melbourne data, is not available in this workspace.  You will need to translate the concepts to work with the data in this notebook, the Iowa data.\n\nCome to the [Learn Discussion](https://www.kaggle.com/learn-forum) forum for any questions or comments. \n\n**Write Your Code Below ... **\n\n"
    },
    {
      "metadata": {
        "_cell_guid": "c7586b1f-ed4e-4fef-92b0-d9cfd8beef10",
        "_uuid": "72b5fd8bb0f0d9e5946980dec6a6dc03d99da43e"
      },
      "cell_type": "markdown",
      "source": "# What this is notebook ... What this notebook is not!\n\nThis workbook is generally about working through the examples within the machine learning track and getting them to work with a secondary data-set plus odd code snippets where I have experimented. The full CRISP-DM process and submission for the:\n\n> **House Prices: Advanced Regression Techniques**    \n> Predict sales prices and practice feature engineering, RFs, and gradient boosting\n\ncompetition will follow in a further notebook where I put all of the skills and topics touched upon here together neatly and efficiently with the aim of creating the best model possible."
    },
    {
      "metadata": {
        "_cell_guid": "5c302d28-f4ea-48a0-86cc-3a7a4cc5c2c0",
        "_uuid": "0fbddf2a4add05ea87f98721fde4eeb0bde29f65"
      },
      "cell_type": "markdown",
      "source": "# Level 1-2: Starting Your ML Project\n\n## Your Turn\n\n**Remember, the notebook you want to \"fork\" is [here](https://www.kaggle.com/dansbecker/my-first-machine-learning-model/).**\n\nRun the equivalent commands (to read the data and print the summary) in the code cell below. The file path for your data is already shown in your coding notebook. Look at the mean, minimum and maximum values for the first few fields. Are any of the values so crazy that it makes you think you've misinterpreted the data?\n\nThere are a lot of fields in this data. You don't need to look at it all quite yet.\n\nWhen your code is correct, you'll see the size, in square feet, of the smallest lot in your dataset. This is from the **min** value of **LotArea**, and you can see the **max** size too. You should notice that it's a big range of lot sizes!\n\nYou'll also see some columns filled with `....` That indicates that we had too many columns of data to print, so the middle ones were omitted from printing.\n\nWe'll take care of both issues in the next step."
    },
    {
      "metadata": {
        "_cell_guid": "86b26423-563a-4fa1-a595-89e25ff93089",
        "_uuid": "1c728098629e1301643443b1341556a15c089b2b",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import relevant libraries and dependencies\nimport pandas as pd\n\n# save filepath to variable for easier access\nmain_file_path = '../input/train.csv'\n\n# read the data and store data in DataFrame titled df\ndf = pd.read_csv(main_file_path)\n\n# print a summary of the data\nprint(df.describe())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "26d3dc52-e105-4629-a11d-fc70bf08996e",
        "_uuid": "bb7012eb05435118b6f8f9245bc9d2945edd8c07"
      },
      "cell_type": "markdown",
      "source": "# Level 1-3: Selecting and Filtering in Pandas\n\n## Your Turn\n\nIn the notebook with your code:\n\n1. Print a list of the columns\n2. From the list of columns, find a name of the column with the sales prices of the homes. Use the dot notation to extract this to a variable (as you saw above to create melbourne_price_data.)\n3. Use the head command to print out the top few lines of the variable you just created.\n4. Pick any two variables and store them to a new DataFrame (as you saw above to create two_columns_of_data.)\n5. Use the describe command with the DataFrame you just created to see summaries of those variables. "
    },
    {
      "metadata": {
        "_cell_guid": "9e684092-ef23-47d3-af07-ce48dc695bde",
        "_uuid": "0f0058044ffb92a419d0d23c8a85777b71df8d22",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Print a list of the columns\nprint(df.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "907f68a3-0d39-4d60-ac8c-237badc0c38d",
        "_uuid": "22faa0a1a181e08c914cba732697a0421f13a5ac",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Extract the sales price using dot notation\nprice_data = df.SalePrice\n\n# Display the head of the variable\nprint(price_data.head(5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ab5a95a3-9419-48cd-b3df-306a723e276a",
        "_uuid": "0b0d9c79c730c17751ecf2ee545ebe762f854539",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Creation of two columns of data\ncolumns_of_interest = [\"YrSold\", \"RoofStyle\"]\n\n# Describe the two columns\nprint(df[columns_of_interest].describe())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c239c0ce-1c2e-4bde-a0c4-913abf571f0e",
        "_uuid": "e99f1a4b97b181d5ad3c305c8e149074db1d62ad"
      },
      "cell_type": "markdown",
      "source": "# Level 1-4: Your First Scikit-Learn Model\n\n## Your Turn\n\nNow it's time for you to define and fit a model for your data (in your notebook).\n\n1. Select the target variable you want to predict. You can go back to the list of columns from your earlier commands to recall what it's called (hint: you've already worked with this variable). Save this to a new variable called y.\n2. Create a **list** of the names of the predictors we will use in the initial model. Use just the following columns in the list (you can copy and paste the whole list to save some typing, though you'll still need to add quotes):\n * LotArea\n * YearBuilt\n * 1stFlrSF\n * 2ndFlrSF\n * FullBath\n * BedroomAbvGr\n * TotRmsAbvGrd\n3. Using the list of variable names you just created, select a new DataFrame of the predictors data. Save this with the variable name X.\n4. Create a DecisionTreeRegressorModel and save it to a variable (with a name like my_model or iowa_model). Ensure you've done the relevant import so you can run this command.\n5. Fit the model you have created using the data in X and the target data you saved above.\n6. Make a few predictions with the model's predict command and print out the predictions."
    },
    {
      "metadata": {
        "_cell_guid": "b7b631e8-a997-4d98-ae59-e4784cfb2102",
        "_uuid": "131fe08cb1909304d9f388c0da43f24e7792e48a",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Select target variable to be predicted\ny = df[\"SalePrice\"]\n\n# Select predictors\nlist_of_predictors = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n\n# Create a predictors dataFrame\nX = df[list_of_predictors]\n\n# Import the decision tree\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Define the model\niowa_model = DecisionTreeRegressor()\n\n# Fit the model\niowa_model.fit(X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d1daf4ef-4660-4545-8244-50938399cb85",
        "_uuid": "fb051c9f926a6c7511afaacbf63aa7b415877b95",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Making a few predictions ...\nprint(\"\\n\" + \"Making predictions for the following 5 houses:\")\nprint(X.head())\n\nprint(\"\\n\" + \"The predictions are\")\nprint(iowa_model.predict(X.head()))\n\nprint(\"\\n\" + \"Compared to their real values:\")\nprint(y.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d80a4cba-b715-467e-9d00-2cfa9f658b80",
        "_uuid": "480d1731077d27701220390688d891b14681bbae"
      },
      "cell_type": "markdown",
      "source": "# Level 1-5: Model Validation\n\n## Your Turn\n\n1. Use the train_test_split command to split up your data.\n2. Fit the model with the training data\n3. Make predictions with the validation predictors\n4. Calculate the mean absolute error between your predictions and the actual target values for the validation data."
    },
    {
      "metadata": {
        "_cell_guid": "90d3179f-9a1f-4c3a-94ab-0d93fe56acb2",
        "_uuid": "e974016527c672499c05ea1c8f155c24d7a31787",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Split data into training and validation data\nfrom sklearn.model_selection import train_test_split\n\n\"\"\"\nN.B.\nThe split is based on a random number generator. Supplying a numeric value to the random_state argument guarantees we get the same split every time we\nrun this script.\n\"\"\"\n\n# Split for training and validation\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n\n# Define the model\niowa_model = DecisionTreeRegressor()\n\n# Fit the model with the training data\niowa_model.fit(train_X, train_y)\n\n# Make predictions on the validation data\nval_predictions = iowa_model.predict(val_X)\n\n# Calculate and print the mean absolute error\nfrom sklearn.metrics import mean_absolute_error\n\nprint(mean_absolute_error(val_y, val_predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "32c1cb3d-c679-4674-bf76-aa7719d798e4",
        "_uuid": "8942bffeb7a75a6eca5af0484ef844453471ae7c"
      },
      "cell_type": "markdown",
      "source": "# Level 1-6: Underfitting, Overfitting and Model Optimization\n\n## Your Turn\n\nIn the near future, you'll be efficient writing functions like `get_mae` yourself. For now, just copy it over to your work area. Then use a for loop that tries different values of `max_leaf_nodes` and calls the `get_mae` function on each to find the ideal number of leaves for your Iowa data.\n\nYou should see that the ideal number of leaves for Iowa data is less than the ideal number of leaves for the Melbourne data. Remember, that a lower MAE is better."
    },
    {
      "metadata": {
        "_cell_guid": "ee1c660d-16da-4fd9-a9f2-92d8878e8607",
        "_uuid": "7776d2c15ae0d7869ab101b26bd8f2b1e557e042",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "09f17530-d73f-4b41-9598-2a32d4e9549f",
        "_uuid": "8adfc73f3814e43b91ca75331bf85ad5f05e17ec",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_choice = 100000 #Arbitrarily chosen\n\n# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in range(5, 5000, 1):\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    \n    if my_mae < best_choice:\n        best_choice = my_mae\n        print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "54f693a8-e2a8-403e-b8aa-e8933f49db6c",
        "_uuid": "aeaccd4552b837d8c7c6c259e47e38d7f9951e0f"
      },
      "cell_type": "markdown",
      "source": "# Level 1-7: Random Forests\n\n## Your Turn\n\nRun the RandomForestRegressor on your data. You should see a big improvement over your best Decision Tree models."
    },
    {
      "metadata": {
        "_cell_guid": "66b3a68b-af18-4fca-84c4-6e006a0d8bfb",
        "_uuid": "e2c9c934040391e81e667564cbdd243623d7789b",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor()\nforest_model.fit(train_X, train_y)\niowa_predictions_random_forest = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, iowa_predictions_random_forest))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d7a2e216-5972-4226-a01f-601c0334c0c6",
        "_uuid": "2c9654acab404b9b684d0110aab4fa6453c84a6c"
      },
      "cell_type": "markdown",
      "source": "# Level 1-8: Submitting from a Kernel"
    },
    {
      "metadata": {
        "_cell_guid": "97731284-58c3-40dd-9102-cd4d5f02dec0",
        "_uuid": "125ab66235ddf93f8400a0e8e1780e19132132d8",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\"\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Read the data\ntrain = pd.read_csv('../input/train.csv')\n\n# pull data into target (y) and predictors (X)\ntrain_y = train.SalePrice\npredictor_cols = ['LotArea', 'OverallQual', 'YearBuilt', 'TotRmsAbvGrd']\n\n# Create training predictors data\ntrain_X = train[predictor_cols]\n\nmy_model = RandomForestRegressor()\nmy_model.fit(train_X, train_y)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e75c723a-af87-4264-be63-017aa689cca2",
        "_uuid": "bb204c246c938ea9de58bd64b33ae1162cfeec85",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n# Read the test data\ntest = pd.read_csv('../input/test.csv')\n# Treat the test data in the same way as training data. In this case, pull same columns.\ntest_X = test[predictor_cols]\n# Use the model to make predictions\npredicted_prices = my_model.predict(test_X)\n# We will look at the predicted prices to ensure we have something sensible.\nprint(predicted_prices)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f1a76923-49bc-4be1-a320-bbf9772b6a81",
        "_uuid": "4ada15157333b95af3dfbc9e532313ce4a39cb59",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\"\nmy_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "638a8955-1d72-41c0-9719-c9442518e357",
        "_uuid": "ce4b339a729a02417dd55c41730fc92b639cefb8",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Level 2-1: Handling Missing Values\n\n## Your Turn\n1) Find some columns with missing values in your dataset.\n\n2) Use the Imputer class so you can impute missing values\n\n3) Add columns with missing values to your predictors.\n\nIf you find the right columns, you may see an improvement in model scores. That said, the Iowa data doesn't have a lot of columns with missing values. So, whether you see an improvement at this point depends on some other details of your model.\n\nOnce you've added the Imputer, keep using those columns for future steps. In the end, it will improve your model (and in most other datasets, it is a big improvement)."
    },
    {
      "metadata": {
        "_cell_guid": "74f85e03-e16b-4032-bb2b-ca6a7e4d6dd0",
        "_uuid": "5d7d6c59f0490424dce50fa691d5c274e38a6224",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import relevant libraries and dependencies\nimport pandas as pd\n\n# save filepath to variable for easier access\niowa_train_path = '../input/train.csv'\n\n# read the data and store data in DataFrame titled df\ndf = pd.read_csv(iowa_train_path)\n\n# print a summary of the data\nprint(df.describe())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "51676c0b-2548-4cd3-b237-e5913cd35c7b",
        "_uuid": "7781ca0ead953ac3b9a66381d3bd609ba8d0289c",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\niowa_target = df.SalePrice\niowa_predictors = df.drop(['SalePrice'], axis=1)\n\n# For the sake of keeping the example simple, we'll use only numeric predictors. \niowa_numeric_predictors = iowa_predictors.select_dtypes(exclude=['object'])\n\nprint(iowa_numeric_predictors.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "98cfc086-1da5-4444-88e5-7aa2bdc6cdee",
        "_uuid": "37b276d569fa9c324e0783e170f3e9a772918a22",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(iowa_numeric_predictors, \n                                                    iowa_target,\n                                                    train_size=0.7, \n                                                    test_size=0.3, \n                                                    random_state=42)\n\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3464b167-dc2d-44d2-a8a5-5031693c1757",
        "_uuid": "2dc6091278919078ebd7d0670212c51d394bce8a",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "cols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\n\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\n\nreduced_X_test  = X_test.drop(cols_with_missing, axis=1)\n\n# Finding (numeric) columns with missing values in the Iowa data set\nprint(\"Columns with missing data:\")\nprint(iowa_numeric_predictors[cols_with_missing].columns)\n\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "07f6ab41-4c29-4703-b9a6-8fa110c4c526",
        "_uuid": "1bf85f62cfaf7074337fa6155bdff13f37cc6201",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import Imputer\n\nmy_imputer = Imputer()\n# See comment section about preserving the structure of the data ...\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_train.columns = X_train.columns\n\nimputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\nimputed_X_test.columns = X_test.columns\n\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ff009431-f9c8-4987-bd0c-d40207046559",
        "_uuid": "c1c9b5d1de6bce6e7fa0eb4770eb51fb8aa27a59",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "imputed_X_train_plus = X_train.copy()\nimputed_X_test_plus = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns \n                                 if X_train[col].isnull().any())\nfor col in cols_with_missing:\n    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n\n# Imputation\nmy_imputer = Imputer()\n# See comment section about preserving the structure of the data ...\nimputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(imputed_X_train_plus))\nimputed_X_train_plus.columns = imputed_X_train_plus.columns\n\nimputed_X_test_plus = pd.DataFrame(my_imputer.transform(imputed_X_test_plus))\nimputed_X_test_plus.columns = imputed_X_test_plus.columns\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ef33a842-45a0-489f-88aa-e2bb6b64801d",
        "_uuid": "ca0b843ba2e94748a7fb250e91c10342637c936b"
      },
      "cell_type": "markdown",
      "source": "# Level 2-2: Using Categorical Data with One Hot Encoding\n\n## Your Turn\nUse one-hot encoding to allow categoricals in your course project. Then add some categorical columns to your X data. If you choose the right variables, your model will improve quite a bit. Once you've done that, Click Here to return to Learning Machine Learning where you can continue improving your model."
    },
    {
      "metadata": {
        "_cell_guid": "5b54e489-1ad3-4c0d-b957-5cec9de92233",
        "_uuid": "d0c353531086130cde80458f61b55b3ae187380c",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Read the data\nimport pandas as pd\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\n\n# Drop houses where the target is missing\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\ntarget = train_data.SalePrice\n\n# Since missing values isn't the focus of this tutorial, we use the simplest\n# possible approach, which drops these columns. \n# For more detail (and a better approach) to missing values, see\n# https://www.kaggle.com/dansbecker/handling-missing-values\ncols_with_missing = [col for col in train_data.columns \n                                 if train_data[col].isnull().any()]                                  \ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncandidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n\n# \"cardinality\" means the number of unique values in a column.\n# We use it as our only way to select categorical columns here. This is convenient, though\n# a little arbitrary.\nlow_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].nunique() < 10 and\n                                candidate_train_predictors[cname].dtype == \"object\"]\n\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n\nmy_cols = low_cardinality_cols + numeric_cols\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "108185a1-c199-4a57-8fee-dc38cb9c963a",
        "_uuid": "fffaaf7b05115c0694730b132a0ba2227de27751",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(train_predictors.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8f5032a4-45b3-400b-9c76-bfd28c3b8445",
        "_uuid": "6434ca9becba3076a1a29113fb8442156f9b47e9",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9ae6f834-f397-4667-99fe-41bd2c20212d",
        "_uuid": "f45acfcf33ada58c7fcc524f08013af5985b96b4",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), \n                                X, y, \n                                scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, target)\n\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n\nprint('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "975a8237-2f84-4fa7-adab-6419502a5260",
        "_uuid": "8f5ec053330c12eee4c534c7c035b4174fe851e0",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8cdfd439-045d-4076-98cd-5992ccc4e55a",
        "_uuid": "70dda7e7de865f762c5bce319cca7e07b62efb15"
      },
      "cell_type": "markdown",
      "source": "# Level 2-3: Learning to use XGBoost\n\n## Your Turn\n\nConvert yuor model to use XGBoost.\n\nUse early stopping to find a good value for n_estimators. Then re-estimate the model with all of your training data, and that value of n_estimators."
    },
    {
      "metadata": {
        "_cell_guid": "82de81d3-be6d-4be3-a331-926213e99038",
        "_uuid": "5a42a85b4fc6dc2405dbaa63a14af413d4d96738",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\n\ndata = pd.read_csv('../input/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a7ec6ad3-a04e-4d6f-a15e-96b44d864437",
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "_uuid": "15ce0e83aea90551ac7bc3094f2d0a64a50e0471",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(train_X, train_y, verbose=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5a114319-3ddf-450d-8293-d93331bb32a5",
        "_uuid": "98fa85a715d91e34ebf40c4c985a2e230dac04a5",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# make predictions\npredictions = my_model.predict(test_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6ecc1bbc-8ab6-4363-bfd1-0155738a74e0",
        "_uuid": "1839170676e1450d14b419b0d2b90469981bf5ad",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "my_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], verbose=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c94291b4-cbbc-42df-8179-1c4dc68cbdbd",
        "_uuid": "747e0e4ea9524f91781712d5c084d2a111dcdef7",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "my_model = XGBRegressor(n_estimators=10000, learning_rate=0.01)\nmy_model.fit(train_X, train_y, early_stopping_rounds=10, \n             eval_set=[(test_X, test_y)], verbose=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a26a2a66-127e-42e6-9a14-da1ad9a1ed8c",
        "_uuid": "ecd4c8bc18d89c32b5d85c07a17ccfb0d9c1a75a",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# make predictions\npredictions = my_model.predict(test_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "09ccda76-f722-4973-bfaf-1f5ee58702eb",
        "_uuid": "ac9eed1a6557ecd434ba05043c1351f07e9868b1"
      },
      "cell_type": "markdown",
      "source": "# Level 2-4: Partial Dependence Plots\n\n## Your Turn\n\nPick three predictors in your project. Formulate an hypothesis about what the partial dependence plot will look like. Create the plots, and check the results against your hypothesis.\n"
    },
    {
      "metadata": {
        "_cell_guid": "be620c16-ad3c-4a69-9ccd-465789d8e39e",
        "_kg_hide-input": false,
        "_uuid": "5ca61dcbe3d85078639a9c722b657af935d4b377",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('../input/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntrain_X = pd.DataFrame(train_X)\ntrain_X.columns = X.columns\n\ntest_X = my_imputer.transform(test_X)\ntest_X = pd.DataFrame(test_X)\ntest_X.columns = X.columns\n\n#print(train_X.info())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e00fe19b-c327-4a70-8039-f677e03b3bba",
        "_uuid": "1b111de04d65d06d8fbf3ff0edf0e0dbc76a97d9",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n\n# scikit-learn originally implemented partial dependence plots only for Gradient Boosting models\n# this was due to an implementation detail, and a future release will support all model types.\nmy_model = GradientBoostingRegressor()\n# fit the model as usual\nmy_model.fit(train_X, train_y)\n# Here we make the plot\nmy_plots = plot_partial_dependence(my_model,       \n                                   features=[0, 2], # column numbers of plots we want to show\n                                   X=train_X,            # raw predictors data.\n                                   feature_names=['LotArea', 'Fireplaces', \"PoolArea\"], # labels on graphs\n                                   grid_resolution=10) # number of values to plot on x axis",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "78c05f52-c506-4a7f-96cd-8054c573d34f",
        "_uuid": "1631fd0cf6baacd822f44ce3bf222f47bc22fcc7"
      },
      "cell_type": "markdown",
      "source": "# Level 2-5: Pipelines\n\n## Your Turn\n\nTake your modeling code and convert it to use pipelines. For now, you'll need to do one-hot encoding of categorical variables outside of the pipeline (i.e. before putting the data in the pipeline).\n\n"
    },
    {
      "metadata": {
        "_cell_guid": "713714f5-5112-4c54-9952-632cf92e3e1e",
        "_uuid": "2721eda995884acea507fbb7417403e0226668c6",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read Data\ndata = pd.read_csv('../input/train.csv')\n\ncols_with_missing = [col for col in data.columns if data[col].isnull().any()]\n\ndata = data.drop(cols_with_missing, axis=1)\n\n# Drop target to get predictors\npredictors = data.drop(['SalePrice'], axis=1)\n\n# Exclude objects\nX_numeric = predictors.select_dtypes(exclude=['object'])\nX_objects = predictors.select_dtypes(include=['object'])\n\none_hot_encoded_training_predictors = pd.get_dummies(X_objects)\n\n#X = X_numeric + one_hot_encoded_training_predictors\nX = pd.concat([X_numeric, one_hot_encoded_training_predictors], axis=1)\n\n# Define target\ny = data.SalePrice\n\n# Train-Test split\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5a783898-c606-46f5-afd3-ce437e88c2b4",
        "_uuid": "594e71aeca6d12abd40725036531898fcd41ee91",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\n\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "70efb819-d39d-4b65-ac4f-7c7558d35269",
        "_uuid": "11a13b6e5e35be9adf8847eb3bbac978cbbcff51",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "my_pipeline.fit(train_X, train_y)\n\npredictions = my_pipeline.predict(test_X)\n\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "869b3a14-fbeb-4a00-b895-91f626ec24df",
        "_uuid": "b8c07e6129aa7c6275344fc264754d6fa5bbe3b9"
      },
      "cell_type": "markdown",
      "source": "# Level 2-6: Cross-Validation\n\n## Your Turn\n\nConvert the code for your on-going project over from train-test split to cross-validation. Make sure to remove all code that divides your dataset into training and testing datasets. Leaving code you don't need any more would be sloppy.\n\nAdd or remove a predictor from your models. See the cross-validation score using both sets of predictors, and see how you can compare the scores."
    },
    {
      "metadata": {
        "_cell_guid": "7317f4b1-1b3e-4a0f-96be-46a053014b9f",
        "_uuid": "1ff08f01745cc4ccafcd78c3c56bdb483ae49c41",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Read Data\ndata = pd.read_csv('../input/train.csv')\n\ncols_with_missing = [col for col in data.columns if data[col].isnull().any()]\n\ndata = data.drop(cols_with_missing, axis=1)\n\n# Drop target to get predictors\npredictors = data.drop(['SalePrice'], axis=1)\n\n# Exclude objects\nX_numeric = predictors.select_dtypes(exclude=['object'])\nX_objects = predictors.select_dtypes(include=['object'])\n\none_hot_encoded_training_predictors = pd.get_dummies(X_objects)\n\nX = X_numeric + one_hot_encoded_training_predictors\nX = pd.concat([X_numeric, one_hot_encoded_training_predictors], axis=1)\n\n# Define target\ny = data.SalePrice",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "e6a254b81d9c0f3f8a81ee01b8826dfd6ed55643"
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\n\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e37b3553bedd0df8bd58f79dde930c32fb29aa0"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_val_score\n\nscores_numeric = cross_val_score(my_pipeline, X_numeric, y, scoring='neg_mean_absolute_error', cv=5)\nscores_object = cross_val_score(my_pipeline, one_hot_encoded_training_predictors, y, scoring='neg_mean_absolute_error', cv=5)\nscores_combined = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error', cv=5)\n\nprint(\"Numeric scores:\", scores_numeric)\nprint(\"Object scores:\", scores_object)\nprint(\"Combined scores:\", scores_combined)",
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Numeric scores: [-19294.5890411  -18679.07465753 -18764.49246575 -17220.42534247\n -21139.03150685]\nObject scores: [-27048.75884306 -31513.93325312 -33642.66630708 -28467.02611412\n -28587.48122895]\nCombined scores: [-18611.03561644 -19121.68321918 -18443.63013699 -17364.38938356\n -21865.44726027]\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2187abeb6e155619ac0e1c4a8a7d01432bbabc0"
      },
      "cell_type": "code",
      "source": "print(\"Mean Absolute Error _ Numeric %2f\" %(-1 * scores_numeric.mean()))\nprint(\"Mean Absolute Error _ Objects %2f\" %(-1 * scores_object.mean()))\nprint(\"Mean Absolute Error _ Combined %2f\" %(-1 * scores_combined.mean()))",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Mean Absolute Error _ Numeric 19019.522603\nMean Absolute Error _ Objects 29851.973149\nMean Absolute Error _ Combined 19081.237123\n"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0dac3995e6014f8b8beda336ff0ad32a8c58b0a8"
      },
      "cell_type": "markdown",
      "source": "# Level 2-7: Data Leakage\n\n## Exercise\n\nReview the data in your ongoing project. Are there any predictors that may cause leakage? As a hint, most datasets from Kaggle competitions don't have these variables. Once you get past those carefully curated datasets, this becomes a common issue."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "46c46e007c4e2648846df2fcdfdae26f90165cc3"
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Read Data\ndata = pd.read_csv('../input/train.csv')",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e29ca3c8ff10eece89ae954da5b5248ad214f63"
      },
      "cell_type": "code",
      "source": "print(data.columns)\n",
      "execution_count": 17,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'Index' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1ea67ffb509c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'Index' object is not callable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "fa4b8d557024a20f1cb72cf9273f738ea86f2ebe"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}